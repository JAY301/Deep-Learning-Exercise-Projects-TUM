{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"i2dl","language":"python","name":"i2dl"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"3_recurrent_neural_networks.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"8rlUmmFfJ0N9","colab_type":"text"},"source":["Recurrent Neural Networks (RNN)\n","====================\n","\n","In this exercise we will work with Recurrent Neural Networks (RNN). A RNN is class of neural networks where the output not only depends on the current input but also on previous inputs along a given input sequence. This allows to exhibit temporal dynamic behaviour and contextual information in a sequence. Common applications for RNN are:\n","\n","- time series analysis\n","- speech recognition\n","- machine translation\n","- image captioning\n","\n","\n","Goal of this exercise\n","========\n","\n","This exercise notebook should help you to experiment how recurrent neural networks are implemented, trained, and used for computer vision problems. Therefore, this notebook is structured as follows:\n","1. Implement your own simple RNN class in Pytorch.\n","2. Explore the backpropagation of the gradients in the RNN and discuss the vanishing gradient problem.\n","3. Implement your own LSTM (Long-Short Term Memory) Network and show that this architecture improves the vanishing gradient problem.\n","4. Build a RNN classifier for the MNIST dataset and train your model.\n","5. Tune the hyperparameters of your model and submit your best model to the server to get bonus points.\n","\n"]},{"cell_type":"code","metadata":{"id":"NSoDHd5AJ_vS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"c96a2482-f8ab-4769-ea75-8672b049c1fd","executionInfo":{"status":"ok","timestamp":1579560772990,"user_tz":-60,"elapsed":23133,"user":{"displayName":"Jay Parmar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBnXq-DXInmUQi33YCW1yBn2YcBuQ75rlXok8uegA=s64","userId":"04956736468405767144"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/i2dl/exercise_3'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vn5W_5H2J_1w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"de96ef73-232d-4929-f97d-77053492cf89","executionInfo":{"status":"ok","timestamp":1579560775080,"user_tz":-60,"elapsed":473,"user":{"displayName":"Jay Parmar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBnXq-DXInmUQi33YCW1yBn2YcBuQ75rlXok8uegA=s64","userId":"04956736468405767144"}}},"source":["%cd 'gdrive/My Drive/i2dl/exercise_3'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/i2dl/exercise_3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gqXM-i2UJ0N_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"fe4788c6-f352-487d-fdd6-52363d96fed1","executionInfo":{"status":"ok","timestamp":1579560776043,"user_tz":-60,"elapsed":354,"user":{"displayName":"Jay Parmar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBnXq-DXInmUQi33YCW1yBn2YcBuQ75rlXok8uegA=s64","userId":"04956736468405767144"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.autograd import Variable\n","import os\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FeJJ7oQtJ0OD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"4d4ac903-68a6-4966-f9ae-74e23d39a8ed","executionInfo":{"status":"ok","timestamp":1579560790967,"user_tz":-60,"elapsed":627,"user":{"displayName":"Jay Parmar","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBnXq-DXInmUQi33YCW1yBn2YcBuQ75rlXok8uegA=s64","userId":"04956736468405767144"}}},"source":["import platform\n","print('Using python: ', platform.python_version())\n","print('Using torch version: ', torch.__version__)\n","print('Using device: ', device)\n","# Machine: 2015 13\" Macbook Pro, i5 dual core"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Using python:  3.6.9\n","Using torch version:  1.3.1\n","Using device:  cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2-j1Q0LxJ0OG","colab_type":"text"},"source":["## Simple Recurrent Neural Network\n","\n","The recurrent loops in a RNN allow relevant information to persist over time. A simple RNN architecture is shown here:\n","<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png width=\"150\">\n","\n","A simple RNN takes not only an input X at time step t but also passes a hidden state that is the output of the previous time step into the network. The output of a RNN cell at time step t reads in Eq. 1:\n","\n","![image.png](attachment:image.png)\n","\n","In this task you have to implement a simple one-layer RNN as a class in Pytorch, where you can choose a relu or tanh activation in the cell.You can see the architecture of a simple RNN in the figure below.\n","\n","\n","<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png width=\"600\">\n","\n"]},{"cell_type":"code","metadata":{"id":"A227YFwLJ0OH","colab_type":"code","colab":{}},"source":["# ToDo: Implement the RNN class\n","from exercise_code.rnn.rnn_nn import RNN"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0qfQTa0J0OK","colab_type":"text"},"source":["Luckily, Pytorch already has implemented a simple RNN in their library and you can call the RNN with <code>nn.RNN</code>. We will use the Pytorch RNN function to check if we have built the correct cell and compare the output of both functions. We also compare the running time of both classes."]},{"cell_type":"code","metadata":{"id":"Txr4EweNJ0OL","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import timeit\n","\n","# choose your network parameters\n","input_size = 3\n","hidden_dim = 3\n","seq_len = 10 \n","\n","# define the two models\n","pytorch_rnn=nn.RNN(input_size, hidden_dim)\n","i2dl_rnn=RNN(input_size, hidden_dim)\n","\n","# initialise both rnn with same values\n","for p in pytorch_rnn.parameters():\n","    nn.init.constant_(p, val=0.3)\n","for p in i2dl_rnn.parameters():\n","    nn.init.constant_(p, val=0.3)\n","    \n","X=torch.randn(seq_len, 1, input_size)\n","\n","output_pytorch, h_pytorch = pytorch_rnn(X)\n","output_i2dl, h_i2dl = i2dl_rnn(X)\n","\n","\n","# The difference of outputs should be 0!!\n","\n","diff = torch.sum((output_pytorch-output_i2dl) ** 2)\n","print(\"Differnce between pytorch and your RNN implementation: %s\" % diff.item())\n","if diff.item() < 10 ** -10:\n","    print(\"Cool, you implemented a correct model.\")\n","else:\n","    print(\"Upps! There is something wrong in your model. Try again!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AB6WrlwDJ0ON","colab_type":"code","colab":{}},"source":["import timeit\n","runs=10 ** 4\n","\n","print(\"Time Pytorch RNN {} runs: {:.3f}s\".format(runs, timeit.timeit(\"pytorch_rnn(X)\", \n","                                       setup=\"from __main__ import pytorch_rnn, X\", \n","                                       number=runs))\n","     )\n","\n","print(\"Time I2DL RNN {} run: {:.3f}s\".format(runs, timeit.timeit(\"i2dl_rnn(X)\", \n","                                       setup=\"from __main__ import i2dl_rnn, X\", \n","                                       number=runs))\n","     )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IjPLeDNaJ0OR","colab_type":"text"},"source":["From now on we will use the Pytorch module that is faster and optimised in performance. However, it is always a good exercise to build the functions by yourself and we really advice you to do the exercise!"]},{"cell_type":"markdown","metadata":{"id":"U8FMQJ_SJ0OS","colab_type":"text"},"source":["## Vanishing Gradient\n","\n","As discussed in the lecture, the simple RNN suffers from vanishing gradients in the backpropagation. The hidden state is manipulated in every time step along the sequence and the effect of the past inputs to the final output vanishes with the distance in time. In the next cell we will explore the vanishing effect of previous inputs in the RNN."]},{"cell_type":"code","metadata":{"id":"16jq6CFkJ0OT","colab_type":"code","colab":{}},"source":["############################################################################\n","# TODO: Define a RNN and explore the gradients on the output h_T wrt. the  #\n","# input at time t and plot your result. What behaviour do you observe?     #\n","# Hints:                                                                   #\n","#   - use one input feature                                                #\n","#   - pytorch allows backward() pass wrt. to any vector                    #\n","#   - backward() can only be applied to scalars and not to output tensors  #\n","#   - choose a good representation of the gradient plot                    #\n","############################################################################\n"," \n","    \n","    \n","############################################################################\n","#                             END OF YOUR CODE                             #\n","############################################################################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xp0DtTMXJ0OX","colab_type":"text"},"source":["\n","<div class=\"alert alert-info\">\n","    <h3>Note</h3>\n","    <p>It can be seen that the gradient of the output at time t wrt. to a previous input decreases exponentially. Hence, the final output does not change significantly for changes in the previous input and hence the RNN does not have memory.</p> \n","<h3>Question</h3> \n","<p>In order to better understand the vanishing gradient problem, calculate the gradients \n","dh_t/dV, dh_t /dW, and dh_t/dX_0 analytically for t=3 and h_0=0 using Eq. 1. This exercise might seem a little bit tedious but it is really useful. Can you explain the vanishing gradient mathematically based on your findings?</p>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"2yS0qtlzJ0OY","colab_type":"text"},"source":["## Long-Short Term Memory Network (LSTM)\n","The vanishing gradient problem had been known for some time until Schmidhuber (1997) developed the Long-Short Term Memory Network and showed that this architecture can overcome the problem. <br> \n","A LSTM is a more advanced recurrent network architecture that is able to learn long time dependencies. The architecture of a LSTM is composed of a forget, input, and output gate and the cell can remember values over arbitrary time intervals. Despite various different and exotic LSTM architectures, the standard LSTM cell is shwon in the figure below:\n","\n","\n","<img src=http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png width=\"600\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZCJBLdEqJ0OZ","colab_type":"text"},"source":["Compared to a simple RNN the LSTM cell has a hidden vector and an additional cell state vector. __What size does the cell state have?__ <br>\n","The operations inside the LSTM are given as \n","\n","<img src=https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1  width=\"600\">\n","where \n","f_t: forget gate,  <br>\n","i_t: input gate, <br>\n","o_t: output gate, <br>\n","h_t: hidden state vector, <br>\n","c_t: cell state vector, <br>\n","x_t: input vector, <br>\n","t is time step, \n","<br> \n","<br> \n","and<br> \n","sigma_g: sigmoid activation <br> \n","sigma_c and sigma_h: hyperbolic tangent function\n"]},{"cell_type":"markdown","metadata":{"id":"GkpECoCmJ0Oa","colab_type":"text"},"source":["In the next step you should implement your own LSTM with the operations stated above."]},{"cell_type":"code","metadata":{"id":"84jGAt8wJ0Ob","colab_type":"code","colab":{}},"source":["# ToDo: Implement the RNN class\n","from exercise_code.rnn.rnn_nn import LSTM"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"82fhbyqUJ0Oe","colab_type":"code","colab":{}},"source":["\n","# choose your input parameters\n","input_size = 3\n","hidden_dim = 3\n","seq_len = 10 \n","\n","# define the two models\n","pytorch_lstm=nn.LSTM(input_size, hidden_dim)\n","i2dl_lstm=LSTM(input_size, hidden_dim)\n","\n","# initialise both lstms with same values\n","for p in pytorch_lstm.parameters():\n","    nn.init.constant_(p, val=0.3)\n","for p in i2dl_lstm.parameters():\n","    nn.init.constant_(p, val=0.3)\n","    \n","X=torch.randn(seq_len, 1, input_size)\n","\n","output_pytorch, (h_pytorch, _) = pytorch_lstm(X)\n","output_i2dl , (h_i2dl, _ )= i2dl_lstm(X)\n","\n","# The difference of outputs should be 0!!\n","diff = torch.sum((output_pytorch-output_i2dl) ** 2)\n","print(\"Differnce between pytorch and your RNN implementation: %s\" % diff.item())\n","if diff.item() < 10 ** -10:\n","    print(\"Cool, you implemented a correct model.\")\n","else:\n","    print(\"Upps! There is something wrong in your model. Try again!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGuesECyJ0Oh","colab_type":"code","colab":{}},"source":["import timeit\n","runs=10 ** 4\n","\n","print(\"Time Pytorch LSTM {} runs: {:.3f}s\".format(runs, timeit.timeit(\"pytorch_lstm(X)\", \n","                                       setup=\"from __main__ import pytorch_lstm, X\", \n","                                       number=runs))\n","     )\n","\n","print(\"Time I2DL LSTM {} runs: {:.3f}s\".format(runs, timeit.timeit(\"i2dl_lstm(X)\", \n","                                       setup=\"from __main__ import i2dl_lstm, X\", \n","                                       number=runs))\n","     )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MPdqNI43J0Ok","colab_type":"text"},"source":["## Explore Gradients \n","Analogously to the RNN, calculate the gradients of the input wrt. to the output of the LSTM and compare it against the RNN gradients. __What do you see?__\n"]},{"cell_type":"code","metadata":{"id":"E-8-zaTzJ0Ok","colab_type":"code","colab":{}},"source":["############################################################################\n","# TODO: Define a RNN and LSTM and explore the gradients on the output h_T   #\n","# wrt. the input at time t and plot your result.                           #\n","############################################################################\n","\n","\n","\n","############################################################################\n","#                             END OF YOUR CODE                             #\n","############################################################################"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KO0OBRHEJ0On","colab_type":"text"},"source":["## MNIST image classification with RNNs\n","\n","In the previous exercises we already have classified images with a Fully Connected and Convolutional Network. In this exercise, we will solve the problem of image classification with a recurrent neural network.  \n","\n","For the experiment we use the MNIST handwritten digits dataset which we already know from the autoencoder exercise. This dataset consists of images of the 10 different digits (10 classes). The images have the resolution 28 x 28. The idea for the RNN classifier is to interpret the image as a sequence of rows. This means that we pass the rows through the RNN and use the final hidden state for classification. "]},{"cell_type":"markdown","metadata":{"id":"LRzy_B0dJ0Oo","colab_type":"text"},"source":["\n","<div class=\"alert alert-info\">\n","    <h3>Note</h3>\n","    <p>\n","    In this semester you have seen three different types of neural networks, namely Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and now Recurrent Neural Networks (RNNs). We have seen that we can use all three architectures for image classification. However, it turned out that some models are better than others for image classification. Try to think about advantages and disadvantages of the models, regarding # of parameters, transformations of the object in the image (scaling, rotation, translation,...), training time, testing time, over-fitting, etc."]},{"cell_type":"code","metadata":{"id":"8_6jb3A7J0Op","colab_type":"code","colab":{}},"source":["# Define data loader\n","from torchvision import transforms\n","import pickle\n","\n","class Unsqueeze(object):\n","    \"\"\"Adds a channel dimension that that our 2 dimensional input (H, W), \n","    fits the 3 dimensional (H, W, C) expectations of pytorch's ToTensor function which\n","    expects a PIL image. This is very inefficient but you most probably will use pytorch's\n","    PIL image loader. Check out the documentation and make it more efficient :)\n","    \"\"\"\n","    def __init__(self, dimension=0):\n","        self.dimension = dimension\n","    def __call__(self, numpy_array):\n","        extended_array = np.expand_dims(numpy_array, self.dimension)\n","        return extended_array\n","    def __repr__(self):\n","        return self.__class__.__name__ + 'dimension={}'.format(dimension)\n","\n","    \n","# transformation of data\n","transform = transforms.Compose([\n","    Unsqueeze(dimension=3),     \n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5,), std=(0.5,))\n","])\n","\n","\n","class MnistDataset(torch.utils.data.Dataset):\n","    def __init__(self, images, labels, \n","                 transform=None):\n","        super(MnistDataset, self).__init__()\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","       \n","    def __len__(self):\n","        return len(self.images)\n","    \n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        label = self.labels[idx]\n","        if self.transform: \n","            image = self.transform(image)\n","        return image, label\n","\n","    \n","# loading the train data\n","with open(\"../datasets/mnist/mnist_train.p\", \"rb\") as f:\n","    mnist_raw = pickle.load(f)\n","\n","X, y = mnist_raw\n","############################################################################\n","# TODO: Set a useful training/ validation split                            #\n","############################################################################    \n","\n","\n","\n","############################################################################\n","#                             END OF YOUR CODE                             #\n","############################################################################\n","\n","\n","train_dset = MnistDataset(X[:int(len(X) * train_split)], y[:int(len(X) * train_split)], transform=transform)\n","val_dset = MnistDataset(X[int(len(X) * train_split):], y[int(len(X) * train_split):], transform=transform)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jeQmytpJ0Os","colab_type":"code","colab":{}},"source":["# Visualize some examples from the dataset. Stolen from other notebooks\n","# We show a few examples of training images from each class.\n","X=train_dset.images\n","y=train_dset.labels\n","\n","\n","classes = list(range(10))\n","num_classes = len(classes)\n","samples_per_class = 5\n","for y_hat, cls in enumerate(classes):\n","    idxs = np.flatnonzero(train_dset.labels == y_hat)\n","    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n","    for i, idx in enumerate(idxs):\n","        plt_idx = i * num_classes + y_hat + 1\n","        plt.subplot(samples_per_class, num_classes, plt_idx)\n","        plt.imshow(X[idx])\n","        plt.axis('off')\n","        if i == 0:\n","            plt.title(cls)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YO0sx5-NJ0Ou","colab_type":"text"},"source":["Build a classifier based on a RNN where you sequentially feed the rows in the network and use the final hidden state for prediction."]},{"cell_type":"markdown","metadata":{"id":"OYSQ71FRJ0Ov","colab_type":"text"},"source":["<img src=https://cdn-images-1.medium.com/max/800/1*Cm_c-I02rBa1rtLZXBhNUw.png width=\"600\">\n"]},{"cell_type":"code","metadata":{"id":"a6PHvn3GJ0Ow","colab_type":"code","colab":{}},"source":["from exercise_code.rnn.rnn_nn import LSTM_Classifier, RNN_Classifier\n","model_rnn = LSTM_Classifier()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tScUldtRJ0O0","colab_type":"code","colab":{}},"source":["from exercise_code.rnn.solver import Solver"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"757kdVodJ0O5","colab_type":"code","colab":{}},"source":["batch_size = 32\n","train_loader = torch.utils.data.DataLoader(\n","                 dataset=train_dset,\n","                 batch_size=batch_size,\n","                 shuffle=True)\n","val_loader = torch.utils.data.DataLoader(\n","                dataset=val_dset,\n","                batch_size=batch_size,\n","                shuffle=False)\n","\n","solver = Solver(optim_args={\"lr\": 1e-3})\n","\n","# train rnn model\n","solver.train(model_rnn, train_loader, val_loader, log_nth=50, num_epochs=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XE2UYMDRJ0O9","colab_type":"text"},"source":["Train your RNN classifier and try to tune the hyperparameters. With your simple RNN classifier you should exceed an accuracy higher than __90%__."]},{"cell_type":"markdown","metadata":{"id":"CtSdsBjRJ0O-","colab_type":"text"},"source":["Try to improve your model by using a LSTM."]},{"cell_type":"code","metadata":{"id":"z7tUoZ8yJ0O_","colab_type":"code","colab":{}},"source":["from exercise_code.rnn.rnn_nn import LSTM_Classifier\n","model= LSTM_Classifier()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nKQlJ5yMJ0PG","colab_type":"text"},"source":["Train your LSTM model again and see wether it improves performance on the validation set"]},{"cell_type":"markdown","metadata":{"id":"2Bn493avJ0PI","colab_type":"text"},"source":["# Test your Model\n","When you are satisfied with your training, you can save the model. In order to be eligible for the bonus points you have to achieve a score higher than __97__."]},{"cell_type":"markdown","metadata":{"id":"M28LIH-lJ0PL","colab_type":"text"},"source":["## Save the Model\n","\n","When you are satisfied with your training, you can save the model."]},{"cell_type":"code","metadata":{"id":"FKx4VM8zJ0PM","colab_type":"code","colab":{}},"source":["os.makedirs('models', exist_ok=True)\n","model_rnn.save(\"models/rnn_mnist_nn.model\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jkg01AvfJ0PP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}